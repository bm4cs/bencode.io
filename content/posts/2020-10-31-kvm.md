---
layout: post
title: "KVM virtualisation"
slug: "kvm"
date: "2020-10-31 20:56:23"
lastmod: "2020-10-31 20:56:25"
comments: false
categories:
    - linux
tags:
    - vm
    - kvm
    - virtualisation
---

The Kernel Virtual Machine is a hypervisor for Linux on hardware with virtualization extensions (Intel VT or AMD-V). It is deployed as a loadable kernel modules, `kvm.ko`, and either `kvm-intel.ko` or `kvm-amd.ko`.

The [KVM Debian Wiki](https://wiki.debian.org/KVM) rocks, and provides details on the basics including a great performance tuning section.


<!-- vim-markdown-toc GFM -->

* [Install](#install)
* [Administration tasks](#administration-tasks)
    * [User specific vs system wide VMs](#user-specific-vs-system-wide-vms)
    * [List VMs](#list-vms)
    * [Start VM](#start-vm)
    * [Shutdown VM](#shutdown-vm)
    * [Murder (hung) VM](#murder-hung-vm)
    * [Autostart default NATed bridged network](#autostart-default-nated-bridged-network)
    * [What if the `default` network interface is not listed](#what-if-the-default-network-interface-is-not-listed)
    * [How to extend / increase a partition](#how-to-extend--increase-a-partition)
    * [Use network ISO source for new VMs](#use-network-iso-source-for-new-vms)
    * [Windows VM disk driver](#windows-vm-disk-driver)

<!-- vim-markdown-toc -->

# Install

Easy instructions to get QEMU/KVM and virt-manager up and running on Arch.

1.  Check CPU supports virtualisation `grep -E "(vmx|svm)" --color=always /proc/cpuinfo`
2.  Make sure VT CPU extension is enabled in BIOS.
3.  User access to `/dev/kvm` is required, so add users into kvm(78) group with `sudo gpasswd -a USER_NAME kvm`
4.  Loading kernel modules `kvm_intel` or `kvm_amd` depend on your CPU, Add module name in `/etc/modules-load.d/kvm.conf` either `kvm_intel` or `kvm_amd`
5.  Install `qemu`, `virt-manager`, `dnsmasq` and `iptables` with `sudo pacman -S qemu virt-manager dnsmasq iptables ebtables dnsmasq`
6.  Run and enable boot up start `libvirtd` daemon with `systemctl start libvirtd` and `systemctl enable libvirtd`
7.  Use PolicyKit authorization create `/etc/polkit-1/rules.d/50-libvirt.rules` with the example policy below.
8.  Create the `libvirt` group and add users with `groupadd libvirt` then `sudo gpasswd -a USER_NAME libvirt`
9.  Check network interface status `sudo virsh net-list --all`. If it is `inactive` start it using `sudo virsh net-start default`
10. Now you can use `virt-manager` GUI to build some VMs.

PolicyKit that allows the `kvm` group to manage libvirt:

```
/* Allow users in kvm group to manage the libvirt
daemon without authentication */
polkit.addRule(function(action, subject) {
    if (action.id == "org.libvirt.unix.manage" &&
        subject.isInGroup("kvm")) {
            return polkit.Result.YES;
    }
});
```

# Administration tasks

## User specific vs system wide VMs

`virsh` when not run as root, will default the connection to libvirt using `qemu://session`, sandboxing the view of VMs per user.

To manage system (i.e. root) level VMs either run `virsh` as root, or run `virsh` with a custom connect string `virsh --connect qemu:///system list --all`.

To change the default bind string can set the `LIBVIRT_DEFAULT_URI` env var.

    $ export LIBVIRT_DEFAULT_URI='qemu:///system'

## List VMs

List user specific VMs:

    $ virsh list --all

Or system wide VMs:

    $ virsh --connect qemu:///system list --all

Or:

    # virsh list --all

## Start VM

    # virsh start ARCHBOX

## Shutdown VM

    # virsh shutdown ARCHBOX

## Murder (hung) VM

    # virsh destroy UBUNTUBOX

## Autostart default NATed bridged network

    sudo virsh net-autostart default

## What if the `default` network interface is not listed

If `virsh net-list` is not listing any network interface just reinitialize it with:

     sudo virsh net-define /usr/share/libvirt/networks/default.xml

## How to extend / increase a partition

1.  Shutdown the VM `virsh shutdown hostname`
2.  Increase the qcow2 image. Find the qcow2 file of the VM and take a backup (just in case). `cp hostname.qcow2 hostname.qcow2.backup` and `qemu-img resize hostname.qcow2 +100GB`
3.  Start the VM `virsh start hostname`
4.  Extend the partition in Window

## Use network ISO source for new VMs

Using the awesome `--location` option:

```
virt-install --virt-type kvm --name buster-amd64 \
--location http://deb.debian.org/debian/dists/buster/main/installer-amd64/ \
--os-variant debian10 \
--disk size=10 --memory 1000
```

## Windows VM disk driver

Check and install fast IO driver `virtio-win` on guest Windows VM.

1. Create new VM guest with below configuration: IDE storage for Windows OS container `WINDOWS.qcow2`, IDE CDROM attach Windows OS ISO.
2. Start VM guest and install the Windows OS as usual
3. Shutdown VM guest
4. Reconfigure VM guest: Add a dummy VirtIO / VirtIO SCSI storage with 100MB size, e.g. `DUMMY.qcow2`, then attach VirtIO driver CD ISO to the IDE CDROM.

5. Restart VM guest
6. Install the VirtIO driver from the IDE CDROM when Windows prompt for new hardware driver
7. For VM guest of Windows 10 and above, run "cmd" as Administrator and run: `bcdedit /set {current} safeboot minimal`
8. Shutdown VM guest
9. Reconfigure VM guest with below configuration: Remove IDE storage for Windows OS DONT delete `WINDOWS.qcow2`, remove VirtIO storage for dummy storage you can delete `DUMMY.qcow2`, remove IDE storage for CD ROM, then add a new VirtIO/VirtIO SCSI storage and attach `WINDOWS.qcow2` to it.
10. Restart the VM guest
11. For VM guest of Windows 10 and above, run command: `bcdedit /deletevalue {current} safeboot`
